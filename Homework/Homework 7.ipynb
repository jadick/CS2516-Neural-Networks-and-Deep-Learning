{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uC56hVPGARsskedxHwV6d8VgyrbxugwG","timestamp":1700001781377},{"file_id":"1yVbZqZN05X5a5M-QnlONlr2S00D_z1r2","timestamp":1698773642202},{"file_id":"1JHfCgl7zh3cgqusqscTE3uMseC49EDNa","timestamp":1616602560792}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ovi3rNd9BeZ4"},"source":["# Homework 7\n","\n","In this homework, you will be using a form of attention called *attention pooling* to solve the \"addition problem\". The addition problem was introduced in the [LSTM paper](https://www.bioinf.jku.at/publications/older/2604.pdf) as a way to test whether an RNN could propagate information across many time steps. In the addition problem, the model is given a sequence of 2D vectors in the format:\n","\n","|     |      |     |     |      |     |      |     |     |     |     |\n","|-----|------|-----|-----|------|-----|------|-----|-----|-----|-----|\n","| 0.5 | -0.7 | 0.3 | 0.1 | -0.2 | ... | -0.5 | 0.9 | ... | 0.8 | 0.2 |\n","| 0   |   0  |  1  |  0  |   0  |     |   0  |  1  |     |  0  |  0  |\n","\n","The first dimension of each vector in the sequence is a random number between 0 and 1. The second dimension is 0 for all entries of the sequence except for 2 of the entries, where it is 1. The goal of the addition problem is to output the sum of the values in the first dimension at the two indices where the second dimension is 1. In the example above, the target would be 0.9 + 0.3 = 1.2. Below is a code snippet that generates a sequence and its target for the addition problem."]},{"cell_type":"code","metadata":{"id":"r42nn-jOxhKp","executionInfo":{"status":"ok","timestamp":1700018602270,"user_tz":300,"elapsed":251,"user":{"displayName":"João Dick","userId":"09906120919510529946"}}},"source":["import numpy as np\n","\n","def addition_problem(sequence_length=50):\n","    output = np.random.uniform(-1, 1, (sequence_length, 2))\n","    output[:, 0] = 0.\n","    random_indices = np.random.choice(sequence_length, size=2, replace=False)\n","    output[random_indices, [0, 0]] = 1\n","    return output, (output[:, 0]*output[:, 1]).sum(keepdims=True)"],"execution_count":1,"outputs":[]},{"cell_type":"code","source":["x,y= addition_problem()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57-_SQB3NbW5","executionInfo":{"status":"ok","timestamp":1700023620538,"user_tz":300,"elapsed":2,"user":{"displayName":"João Dick","userId":"09906120919510529946"}},"outputId":"a37d27fb-da54-40d6-86d3-26f958f962a0"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([50, 2, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ew0FypwYCwpn"},"source":["Attention pooling is a form of attention that allows a model to solve the addition problem without using an RNN. In attention pooling, the query vector $q$ is a *learnable parameter*. The keys and values are both the input sequence. Specifically, given a sequence $\\{h_1, h_2, \\ldots, h_T\\}$, attention pooling computes\n","\\begin{align}\n","e_t &= \\mathrm{a}(q, h_t) \\\\\n","\\alpha_t &= \\frac{\\exp(e_t)}{\\sum_k \\exp(e_k)} \\\\\n","c &= \\sum_{t = 1}^T \\alpha_t h_t\n","\\end{align}\n","where $\\mathrm{a}(q, h_t)$ is the attention energy function. Note that c will always be a fixed-length vector (which amounts to a weighted average of the elements of the sequence $h$) regardless of how long the sequence is (i.e. the value of $T$). $\\mathrm{a}(q, h_t)$ can be any function that takes in a single entry of the sequence $h_t$ and outputs an unnormalizes scalar value. One option is to use\n","$$\\mathrm{a}(q, h_t) = q^\\top \\tanh(W_a h_t + b_a)$$\n","where $q \\in \\mathbb{R}^q$, $W_a \\in \\mathbb{R}^{q \\times d}$, and $b_a \\in \\mathbb{R}^q$ are learnable parameters, and $d$ is the dimensionality of $h_t$ (i.e. $h_t \\in \\mathbb{R}^d$).\n","\n","\n","1. Build and train a neural network that uses attention pooling to solve the addition problem. The model should output a scalar which corresponds to the target value for the addition problem (i.e. the sum of the sequence entries that are marked with a \"1\"). Here, \"solved\" means that the squared error of the model's predicitons is always below $0.05$. Use a sequence length of $50$ (which is the default for the `addition_problem` function defined above). *Hints*:\n","  1. This is a regression problem. Your model should predict a continuous scalar value and you can use a squared-error loss.\n","  1. The point of the attention pooling layer is to allow you to put it in an otherwise feed-forward network. So, consider just using simple dense feed-forward layers before and/or after the attention pooling layer. To start, you can try the architecture: feed-forward, attention pooling, feed-forward, output layer.\n","  1. If you are finding that the model is getting stuck at a non-zero squared error, it could be that it's just outputting the mean value and having trouble learning a good solution. Try different initialization, nonlinearities, architecture, learning rate, etc.\n","1. Once you have trained a model that gets solid performance at sequence length $50$, plot the model's average squared error for sequence lengths $50, 55, 65, 80, 100, 125, 150$. You should generate this plot by averaging the squared error over at least $100$ sequences of a given length. Does the model's error get worse (go up) for longer sequences, or does it generalize to longer sequence lengths?"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","class AttentionPooling(nn.Module):\n","    def __init__(self, input_dim, attention_dim):\n","        super(AttentionPooling, self).__init__()\n","        self.W = nn.Linear(input_dim, attention_dim)\n","        self.q = nn.Parameter(torch.randn(attention_dim))\n","        self.softmax = nn.Softmax()\n","\n","    def forward(self, h):\n","        e_t = torch.matmul(torch.tanh(self.W(h)), self.q)\n","        e_t = e_t.unsqueeze(0) if e_t.dim() == 1 else e_t\n","        #print(e_t.shape)\n","        alpha_t = self.softmax(e_t)\n","        return  torch.sum(h * alpha_t.unsqueeze(-1), dim=1)\n","\n","class AdditionNet(nn.Module):\n","    def __init__(self, input_dim, hidden_dim):\n","        super(AdditionNet, self). __init__()\n","        self.dense1 = nn.Linear(input_dim, hidden_dim)\n","        self.attention = AttentionPooling(hidden_dim, hidden_dim)\n","        self.dense2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.dense3 = nn.Linear(hidden_dim, 1)\n","\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.dense1(x)\n","        #print(x.shape)\n","        x = self.relu(x)\n","        x = self.attention(x)\n","        x = self.dense2(x)\n","        x = self.relu(x)\n","        x = self.dense3(x)\n","        return x\n","\n","if torch.cuda.is_available():\n"," device = \"cuda:0\"\n","else:\n"," device = \"cpu\"\n"],"metadata":{"id":"gzqqN15C-0sc","executionInfo":{"status":"ok","timestamp":1700025787294,"user_tz":300,"elapsed":150,"user":{"displayName":"João Dick","userId":"09906120919510529946"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["X, y = addition_problem()\n","input_dim = X.shape[1]\n","model = AdditionNet(input_dim, 64)\n","model.to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","EPOCHS = 30000\n","validation_loss = []\n","for epoch in range(EPOCHS):\n","\n","    #load training data\n","    X_train, y_train = addition_problem()\n","    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    y_train = torch.tensor([y_train], dtype=torch.float32).to(device)\n","    #train\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs_train = model(X_train)\n","    train_loss = criterion(outputs_train, y_train)\n","    train_loss.backward()\n","    optimizer.step()\n","    #load valdiation data\n","    X_val, y_val = addition_problem()\n","    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n","    y_val = torch.tensor([y_val], dtype=torch.float32).to(device)\n","    #validate\n","    model.eval()\n","    with torch.no_grad():\n","        outputs_val = model(X_val)\n","        val_loss = criterion(outputs_val, y_val)\n","        validation_loss.append(val_loss.item())\n","\n","\n","    if epoch % 1000 == 0:\n","        print(f\"Epoch [{epoch+1}/{EPOCHS}], Training loss: {train_loss.item()}, Validation loss: {val_loss.item()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nEVQwkSPcGr","executionInfo":{"status":"ok","timestamp":1700027450257,"user_tz":300,"elapsed":86130,"user":{"displayName":"João Dick","userId":"09906120919510529946"}},"outputId":"049b7fc3-25e6-4b86-cec1-a7a41ff2b6fb"},"execution_count":160,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30000], Training loss: 0.012641029432415962, Validation loss: 4.016443926957436e-05\n","Epoch [1001/30000], Training loss: 0.043585773557424545, Validation loss: 0.24100616574287415\n","Epoch [2001/30000], Training loss: 0.04439060017466545, Validation loss: 0.015423712320625782\n","Epoch [3001/30000], Training loss: 0.012097177095711231, Validation loss: 0.17862620949745178\n","Epoch [4001/30000], Training loss: 0.006495403125882149, Validation loss: 0.005633289460092783\n","Epoch [5001/30000], Training loss: 2.878401573980227e-05, Validation loss: 0.001853450550697744\n","Epoch [6001/30000], Training loss: 0.0001405157381668687, Validation loss: 0.00021780532551929355\n","Epoch [7001/30000], Training loss: 0.0003618178889155388, Validation loss: 8.520203118678182e-05\n","Epoch [8001/30000], Training loss: 2.4126904463628307e-05, Validation loss: 4.958543513566838e-07\n","Epoch [9001/30000], Training loss: 0.0001601198746357113, Validation loss: 6.0272126575000584e-05\n","Epoch [10001/30000], Training loss: 6.447928171837702e-06, Validation loss: 4.162938421359286e-05\n","Epoch [11001/30000], Training loss: 6.930629751877859e-05, Validation loss: 6.921700696693733e-05\n","Epoch [12001/30000], Training loss: 2.425105094516766e-07, Validation loss: 1.4260981515690219e-05\n","Epoch [13001/30000], Training loss: 1.2304193660384044e-05, Validation loss: 0.0001473269221605733\n","Epoch [14001/30000], Training loss: 0.0007062575314193964, Validation loss: 0.0003813760122284293\n","Epoch [15001/30000], Training loss: 0.00011626830382738262, Validation loss: 8.90809405973414e-06\n","Epoch [16001/30000], Training loss: 0.000382031052140519, Validation loss: 0.0008521955460309982\n","Epoch [17001/30000], Training loss: 1.0967512025672477e-05, Validation loss: 6.538574234582484e-05\n","Epoch [18001/30000], Training loss: 2.6170871933572926e-05, Validation loss: 0.00029693671967834234\n","Epoch [19001/30000], Training loss: 8.530065542800003e-08, Validation loss: 4.0229202568298206e-05\n","Epoch [20001/30000], Training loss: 1.4252204039166827e-07, Validation loss: 3.5537086660042405e-05\n","Epoch [21001/30000], Training loss: 3.109066028628149e-06, Validation loss: 2.7594248876994243e-06\n","Epoch [22001/30000], Training loss: 5.092118499305798e-06, Validation loss: 2.1873749574297108e-05\n","Epoch [23001/30000], Training loss: 7.751869998173788e-05, Validation loss: 0.0004976358613930643\n","Epoch [24001/30000], Training loss: 0.00021452957298606634, Validation loss: 0.00010318683052901179\n","Epoch [25001/30000], Training loss: 1.0472922440385446e-05, Validation loss: 3.7649763271474512e-06\n","Epoch [26001/30000], Training loss: 8.563173651054967e-06, Validation loss: 5.162570232641883e-06\n","Epoch [27001/30000], Training loss: 0.0002780637878458947, Validation loss: 0.0007703825831413269\n","Epoch [28001/30000], Training loss: 1.6420985048171133e-05, Validation loss: 2.9152941351640038e-05\n","Epoch [29001/30000], Training loss: 9.016405783768278e-06, Validation loss: 4.95116546517238e-06\n"]}]},{"cell_type":"markdown","source":["By observing the displayed validation loss logs and validation loss plot we verify that the model achieves $MSE < 0.05$."],"metadata":{"id":"W1gXg3k5Z_8d"}},{"cell_type":"code","source":["plt.plot(validation_loss)\n","plt.axhline(y = 0.05, color = 'r', linestyle = '-')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"ShCUFpoRlT4e","executionInfo":{"status":"ok","timestamp":1700027450257,"user_tz":300,"elapsed":15,"user":{"displayName":"João Dick","userId":"09906120919510529946"}},"outputId":"f01a4b31-a1d7-4d28-f695-ef00d72fa29a"},"execution_count":161,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["lengths= [50,55,65,80,100,125,150]\n","\n","for len in lengths:\n","    MSE = 0\n","    for i in range(100):\n","        x, y = addition_problem(len)\n","        x = torch.Tensor(x).to(device)\n","        y = torch.Tensor(y).to(device)\n","        model.eval()\n","        with torch.no_grad():\n","            output = model(x)\n","            loss = criterion(output, y)\n","            MSE += loss.item()\n","            #print(loss.item())\n","    MSE = MSE/100\n","    print(f\"MSE for length {len} is {MSE}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHg-axw-VUcL","executionInfo":{"status":"ok","timestamp":1700027455048,"user_tz":300,"elapsed":980,"user":{"displayName":"João Dick","userId":"09906120919510529946"}},"outputId":"4c4b9c0c-420b-4003-db37-45199638ab25"},"execution_count":162,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE for length 50 is 3.430476771441704e-05\n","MSE for length 55 is 3.125289208821158e-05\n","MSE for length 65 is 2.6328903410126614e-05\n","MSE for length 80 is 3.403158957309471e-05\n","MSE for length 100 is 2.2096396479478477e-05\n","MSE for length 125 is 2.3590429293491864e-05\n","MSE for length 150 is 3.0195941093893452e-05\n"]}]},{"cell_type":"markdown","source":["We can observe that the average validation loss for 100 different sequences  also achieves $MSE < 0.05$, generalizing for longer sequences."],"metadata":{"id":"v9wWfrr2fZoV"}}]}