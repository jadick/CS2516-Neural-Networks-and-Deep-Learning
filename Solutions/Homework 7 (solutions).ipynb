{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovi3rNd9BeZ4"
      },
      "source": [
        "# Homework 7\n",
        "\n",
        "In this homework, you will be using a form of attention called *attention pooling* to solve the \"addition problem\". The addition problem was introduced in the [LSTM paper](https://www.bioinf.jku.at/publications/older/2604.pdf) as a way to test whether an RNN could propagate information across many time steps. In the addition problem, the model is given a sequence of 2D vectors in the format:\n",
        "\n",
        "|     |      |     |     |      |     |      |     |     |     |     |\n",
        "|-----|------|-----|-----|------|-----|------|-----|-----|-----|-----|\n",
        "| 0.5 | -0.7 | 0.3 | 0.1 | -0.2 | ... | -0.5 | 0.9 | ... | 0.8 | 0.2 |\n",
        "| 0   |   0  |  1  |  0  |   0  |     |   0  |  1  |     |  0  |  0  |\n",
        "\n",
        "The first dimension of each vector in the sequence is a random number between 0 and 1. The second dimension is 0 for all entries of the sequence expect for 2, where it is 1. The goal of the addition problem is to output the sum of the values in the first dimension at the two indices where the second dimension is 1. In the example above, the target would be 0.9 + 0.3 = 1.2. Below is a code snippet which generates a sequence and its target for the addition problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r42nn-jOxhKp"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def addition_problem(sequence_length=50):\n",
        "    output = np.random.uniform(-1, 1, (sequence_length, 2))\n",
        "    output[:, 0] = 0.\n",
        "    random_indices = np.random.choice(sequence_length, size=2, replace=False)\n",
        "    output[random_indices, [0, 0]] = 1\n",
        "    return output, (output[:, 0]*output[:, 1]).sum(keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew0FypwYCwpn"
      },
      "source": [
        "Attention pooling is a form of attention that allows a model to solve the addition problem without using an RNN. In attention pooling, the query vector $q$ is a *learnable parameter*. The keys and values are both the input sequence. Specifically, given a sequence $\\{h_1, h_2, \\ldots, h_T\\}$, attention pooling computes\n",
        "\\begin{align}\n",
        "e_t &= \\mathrm{a}(q, h_t) \\\\\n",
        "\\alpha_t &= \\frac{\\exp(e_t)}{\\sum_k \\exp(e_k)} \\\\\n",
        "c &= \\sum_{t = 1}^T \\alpha_t h_t\n",
        "\\end{align}\n",
        "where $\\mathrm{a}(q, h_t)$ is the attention energy function. Note that c will always be a fixed-length vector (which amounts to a weighted average of the elements of the sequence $h$) regardless of how long the sequence is (i.e. the value of $T$). $\\mathrm{a}(q, h_t)$ can be any function that takes in a single entry of the sequence $h_t$ and outputs an unnormalizes scalar value. One option is to use\n",
        "$$\\mathrm{a}(q, h_t) = q^\\top \\tanh(W_a h_t + b_a)$$\n",
        "where $q \\in \\mathbb{R}^q$, $W_a \\in \\mathbb{R}^{q \\times d}$, and $b_a \\in \\mathbb{R}^q$ are learnable parameters, and $d$ is the dimensionality of $h_t$ (i.e. $h_t \\in \\mathbb{R}^d$).\n",
        "\n",
        "\n",
        "1. Build and train a neural network that uses attention pooling to solve the addition problem. The model should output a scalar which corresponds to the target value for the addition problem (i.e. the sum of the sequence entries that are marked with a \"1\"). Here, \"solved\" means that the squared error of the model's predicitons is always below $0.05$. Use a sequence length of $50$ (which is the default for the `addition_problem` function defined above). *Hints*:\n",
        "  1. This is a regression problem. Your model should predict a continuous scalar value and you can use a squared-error loss.\n",
        "  1. The point of the attention pooling layer is to allow you to put it in an otherwise feed-forward network. So, consider just using simple dense feed-forward layers before and/or after the attention pooling layer. To start, you can try the architecture: feed-forward, attention pooling, feed-forward, output layer.\n",
        "  1. If you are finding that the model is getting stuck at a non-zero squared error, it could be that it's just outputting the mean value and having trouble learning a good solution. Try different initialization, nonlinearities, architecture, learning rate, etc.\n",
        "1. Once you have trained a model that gets solid performance at sequence length $50$, plot the model's average squared error for sequence lengths $50, 55, 65, 80, 100, 125, 150$. You should generate this plot by averaging the squared error over at least $100$ sequences of a given length. Does the model's error get worse (go up) for longer sequences, or does it generalize to longer sequence lengths?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9ZfY19_KKz-"
      },
      "source": [
        "1. It's common to represent a batch of sequences as an array of shape (batch_size, sequence_length, n_features).\n",
        "2.  position-wise feedforward layer (like the ones we discussed in the Transformer) are dense layers that are applied independently to each sequence step for each sequence in the batch. Specifically, you can think of it as computing output[i, j] = dot(W, input[i, j]) + b, where input is of shape (batch_size, sequence_length, n_input) and output is of shape (batch_size, sequence_length, n_output), i indexes the batch dimension, and j indexes the sequence timestep.\n",
        "3. If your dense layer assumes that the input is of shape (batch_size, n_input), you reshape the input to (batch_size*sequence_length, n_input), feed it through the dense layer, and reshape the result back to shape (batch_size, sequence_length, n_output). This is because either way you are applying the dense layer independently to each vector in each sequence.\n",
        "4. The network architecture I suggested is something like dense -> attention pooling -> dense. You can experiment with different network architectures.\n",
        "5. The point of the attention pooling is to transform an input from shape (batch_size, sequence_length, n_features) to (batch_size, n_features). This is done by taking a (weighted) average over the sequence length dimension. Once you perform this transformation, you get a single vector for each input example in the batch, regardless of the sequence length. This allows you to compute a single loss for each example in the batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXAr0ehuD56o"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "\n",
        "def addition_problem(sequence_length=50):\n",
        "    output = np.random.uniform(-1, 1, (sequence_length, 2))\n",
        "    output[:, 0] = 0.\n",
        "    random_indices = np.random.choice(sequence_length, size=2, replace=False)\n",
        "    output[random_indices, [0, 0]] = 1\n",
        "    return torch.tensor(output), torch.tensor((output[:, 0]*output[:, 1]).sum(keepdims=True))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HuyoUklGVXd"
      },
      "source": [
        "def addition_with_batch(sequence_length=50): # size = 4\n",
        "  len = sequence_length\n",
        "  x1, y1 = addition_problem(len)\n",
        "  x2, y2 = addition_problem(len)\n",
        "  x3, y3 = addition_problem(len)\n",
        "  x4, y4 = addition_problem(len)\n",
        "\n",
        "  xx = torch.cat((x1,x2,x3,x4)).reshape(4, len, 2).float()\n",
        "  yy = torch.cat((y1,y2,y3,y4)).reshape(4, 1).float()\n",
        "  return xx, yy\n",
        "x, y = addition_with_batch(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RE_kEuOEBj-"
      },
      "source": [
        "# this is one way to define a network\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, d, q, n_hidden, n_output): # d = 2, q = 10, 10 ,1\n",
        "        super(Net, self).__init__()\n",
        "        self.d = d\n",
        "        self.q = q\n",
        "        self.n_hidden = n_hidden\n",
        "        self.hidden1 = torch.nn.Linear(d,n_hidden)\n",
        "        self.w = torch.nn.Linear(n_hidden,q)\n",
        "        self.query = torch.nn.Parameter(torch.zeros(q,1))\n",
        "        #self.register_parameter(name='query', param=torch.nn.Parameter(torch.zeros(q,1)))\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.hidden2 = torch.nn.Linear(d, n_hidden)   # hidden layer\n",
        "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
        "\n",
        "    def forward(self, x,):\n",
        "        keys = x\n",
        "        values = x\n",
        "\n",
        "        # hidden 1\n",
        "        x = x.reshape(-1,self.d)\n",
        "        x = F.relu(self.hidden1(x))      # activation function for hidden layer\n",
        "        x = x.reshape(4,-1,self.n_hidden)\n",
        "\n",
        "        # attention\n",
        "        x = x.reshape(-1,self.n_hidden)\n",
        "        x = self.w(x)\n",
        "        x = self.tanh(x)\n",
        "        x = torch.matmul(x,self.query)\n",
        "        x = x.reshape(4,-1)\n",
        "        x = torch.nn.functional.softmax(x,dim = 1) # 4*50\n",
        "        x = x.reshape(4,1,-1) # 4*1*50\n",
        "        # value's size is 4*50*2\n",
        "        x = torch.bmm(x,values).reshape(4,2)\n",
        "\n",
        "        # hidden 2\n",
        "        x = F.relu(self.hidden2(x))      # activation function for hidden layer\n",
        "        x = self.predict(x)             # linear output\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBvvQYfOEGQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865e68af-99d1-4384-d06d-2c5601c4b3fc"
      },
      "source": [
        "net = Net(d=2, q=10, n_hidden=10, n_output=1)     # define the network\n",
        "print(net)  # net architecture\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (hidden1): Linear(in_features=2, out_features=10, bias=True)\n",
            "  (w): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (hidden2): Linear(in_features=2, out_features=10, bias=True)\n",
            "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WXwNM72-mIp",
        "outputId": "cc366d9f-a1b7-46ac-e4ce-69082e846c57"
      },
      "source": [
        "prediction = net(x)     # input x and predict based on x\n",
        "\n",
        "loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.2809, grad_fn=<MseLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6TDbfZ7CBLI",
        "outputId": "798e67fb-53f4-4ce8-8211-d74154237f67"
      },
      "source": [
        "# train the network\n",
        "for t in range(2000):\n",
        "    x, y = addition_with_batch()\n",
        "    prediction = net(x)     # input x and predict based on x\n",
        "\n",
        "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
        "\n",
        "    optimizer.zero_grad()   # clear gradients for next train\n",
        "    loss.backward()         # backpropagation, compute gradients\n",
        "    optimizer.step()        # apply gradients\n",
        "    if(t%200 == 0):\n",
        "      print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0609, grad_fn=<MseLossBackward>)\n",
            "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
            "tensor(7.6854e-05, grad_fn=<MseLossBackward>)\n",
            "tensor(3.5542e-05, grad_fn=<MseLossBackward>)\n",
            "tensor(1.5260e-05, grad_fn=<MseLossBackward>)\n",
            "tensor(3.1213e-06, grad_fn=<MseLossBackward>)\n",
            "tensor(2.2107e-06, grad_fn=<MseLossBackward>)\n",
            "tensor(2.2950e-06, grad_fn=<MseLossBackward>)\n",
            "tensor(3.9795e-06, grad_fn=<MseLossBackward>)\n",
            "tensor(1.4647e-06, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4xoQZfxDUpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db4cd45-9b02-4b4e-de26-46efd47da2f4"
      },
      "source": [
        "x, y = addition_with_batch()\n",
        "prediction = net(x.float())\n",
        "print(y)\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.3133],\n",
            "        [-1.6394],\n",
            "        [ 0.6015],\n",
            "        [-0.7667]])\n",
            "tensor([[ 1.3139],\n",
            "        [-1.6392],\n",
            "        [ 0.5988],\n",
            "        [-0.7665]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-wF_wQpoLLG"
      },
      "source": [
        "Once you have trained a model that gets solid performance at sequence length  50 , plot the model's average squared error for sequence lengths  50,55,65,80,100,125,150 . You should generate this plot by averaging the squared error over at least  100  sequences of a given length. Does the model's error get worse (go up) for longer sequences, or does it generalize to longer sequence lengths?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEQRF1FQoW-h"
      },
      "source": [
        "def compute_loss(net,sequence_len,batch_num): # num is num_batch * 4\n",
        "  total_loss = 0\n",
        "  for i in np.arange(batch_num):\n",
        "    x,y = addition_with_batch(sequence_len)\n",
        "    prediction = net(x)\n",
        "    loss = loss_func(prediction, y)\n",
        "    total_loss = total_loss + loss\n",
        "  return total_loss / batch_num\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "G4r9WGBOr3IW",
        "outputId": "4a735315-2d00-4f81-e2ed-8a01d057a4e3"
      },
      "source": [
        "x_axis = [50,55,65,80,100,125,150]\n",
        "y_axis = []\n",
        "for i in x_axis:\n",
        "  y_axis.append(compute_loss(net,i,100))\n",
        "plt.plot(x_axis,y_axis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2c581f17d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMsa_HxYu8rG"
      },
      "source": [
        "# In this case, the model's error gets larger as the sequence length gets larger,\n",
        "# but it's also possible to train a model that generalizes well to longer sequences."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}