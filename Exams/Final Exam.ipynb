{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final exam\n",
        "\n",
        "Please hand in the final exam via MarkUs by 9am on Friday, December 15th. You are welcome to spend any amount of time working on the exam before then. The exam is \"open-course-materials\": you are allowed to consult the textbook (d2l.ai), my lecture notes, class recordings, your own course notes, and past homeworks. You are welcome to check your answers by writing and running code, but this should not be necessary for any of the problems. No collaboration with other students or use of other sources is allowed."
      ],
      "metadata": {
        "id": "5-GuNEDFM0ju"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UKeY-p1lhKr"
      },
      "source": [
        "# Problem 1 (2 points)\n",
        "\n",
        "Show that a softmax nonlinearity with two logits is equivalent to a sigmoid nolinearity. Hint: Write down $p(y = 0)$ and $p(y = 1)$ for the softmax and sigmoid and find an expression for the sigmoid logit in terms of the softmax logits that causes the two to be equivalent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMugG9cnpIM3"
      },
      "source": [
        "# Problem 2 (6 points)\n",
        "\n",
        "Consider minimization of the function $y = \\frac{x^2}{4}$ with respect to x. Assume that the initial value of $x$ is $x_0 = 1$.\n",
        "\n",
        "1. Find the learning rate so that gradient descent converges to the minimum in one iteration.\n",
        "1. Now, consider minimization of this function with a momentum optimizer with the learning rate fixed to $1$. Find the value of the momentum hyperparameter (we called it $\\beta$ in class) so that optimization converges to the minimum in two iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w5T0U_ftC0X"
      },
      "source": [
        "# Problem 3 (3 points)\n",
        "\n",
        "Rewrite the following convolution\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "x_1 & x_2 & x_3 & x_4 \\\\\n",
        "\\end{bmatrix}\n",
        "*\n",
        "\\begin{bmatrix}\n",
        "k_1 & k_2\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "as a matrix-vector product. Assume there is no padding and all strides are 1."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4 (6 points)\n",
        "\n",
        "Consider the vanilla RNN layer:\n",
        "$$h_t = \\phi(W_x x_t + W_h h_{t - 1} + b)$$\n",
        "For the purposes of this problem, assume that $W_x = W_h = I$ (i.e. both weight matrices are the identity matrix), $b = 0$, and $\\phi(x) = x$ (i.e. there is no nonlinearity).\n",
        "\n",
        "1. Provide a closed-form expression for $h_t$ in terms of $x_1, \\ldots, x_t$.\n",
        "1. Provide a closed-form expression for $\\frac{\\partial h_t}{\\partial W_h}$ in terms of $h_0, \\ldots, h_t$."
      ],
      "metadata": {
        "id": "AEJuQoMsick9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh6aywWqj6pW"
      },
      "source": [
        "# Problem 5 (2 points)\n",
        "\n",
        "In class and in the textbook, we introduced Dropout as replacing an activation $h_i$ with\n",
        "$$\n",
        "h_i' = \\begin{cases}\n",
        "0\\;\\text{with probability}\\;p\\\\\n",
        "\\frac{h_i}{1 - p}\\;\\text{with probability}\\;1 - p\n",
        "\\end{cases}\n",
        "$$\n",
        "An alternative formulation is\n",
        "$$\n",
        "h_i' = \\begin{cases}\n",
        "0 \\;\\text{with probability}\\;p\\\\\n",
        "h_i \\;\\text{with probability}\\;1 - p\n",
        "\\end{cases}\n",
        "$$\n",
        "Assume that this alternative form of dropout is applied before a standard fully-connected (dense) layer. How should the weights of the fully-connected layer be modified so that when dropout is not applied the expected output of the layer is unchanged?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQDWaNSE9-WZ"
      },
      "source": [
        "# Problem 6 (2 points)\n",
        "\n",
        "I decide that ReLU(x) increases so quickly so I try a new nonlinearity: $\\mathrm{LogLU}(x) = \\log{\\mathrm{ReLU}(x)}$. I figure I'm being clever by applying the ReLU before the log so the output is always finite. I train a model using this nonlinearity and immediately get NaNs. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOI714Yb55Fv"
      },
      "source": [
        "# Problem 7 (9 points)\n",
        "\n",
        "For this problem, consider a single-headed local self-attention mechanism we briefly discussed in class, i.e. one where the attention weight for any value more than $k$ timesteps away from the query is set to zero.\n",
        "\n",
        "1. Write down a modified version of standard self-attention that implements local self-attention. You should assume that standard self-attention uses the scaled dot-product comparison function and a softmax-weighted average for the reduction function. As usual, you should define what the queries, keys, values, comparison function, and reduction function are, though you can feel free to just state that any of these these are computed the same as in standard self-attention.\n",
        "\n",
        "1. Write down the number of times $\\alpha$ needs to be called to compute the output of standard self-attention and local self-attention. What is the relative speedup or slowdown of using local self-attention?\n",
        "\n",
        "1. Assume that the input to the single-headed local self-attention mechanism is a one-dimensional sequence. Modify queries, keys, and/or values for local self-attention so that it closely approximates max-pooling with a stride of 1 and a window size of $2k + 1$ around the current input. You must use the same definition for the comparison function and reduction function as in part 1."
      ]
    }
  ]
}