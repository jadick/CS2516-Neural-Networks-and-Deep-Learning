{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAvDZVet2aFn"
      },
      "source": [
        "# Midterm\n",
        "\n",
        "Please hand in the midterm via MarkUs by 9:00am on Thursday, October 26th. You are welcome to spend any amount of time working on the midterm before then. The midterm is \"open-course-materials\": you are allowed to consult the textbook (d2l.ai), my lecture notes, class recordings, your own course notes, past homeworks, and tutorial materials. You are welcome to check your answers by writing and running code, but this should not be necessary for any of the problems. No collaboration with other students or use of other sources is allowed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_a4wrpy8k4_"
      },
      "source": [
        "# Problem 1 (2 points)\n",
        "\n",
        "In class, we introduced the weight decay regularizer, which adds a loss term $\\lambda ||w||_2^2$ where $w$ is a vector comprising all of the parameters of the model and $\\lambda$ is a scalar hyperparameter. An alternative is L1 regularization, which adds the loss term $\\lambda |w|$. Let the model's original, unregularized loss be $L$, so that the regularized loss is $L + \\lambda |w|$.\n",
        "1. What is the gradient of the L1 regularization term with respect to the parameters $w$? For simplicity, you can assume the model has only one parameter, so that $w$ is a scalar.\n",
        "1. What is the parameter update under normal gradient descent for $L + \\lambda |w|$? Since you don't know $L$, you can include a $\\nabla_w L$ term in your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5DiSpXZ-E33"
      },
      "source": [
        "# Problem 2 (6 points)\n",
        "\n",
        "Consider the following model and loss function:\n",
        "\\begin{align}\n",
        "h &= \\text{ReLU}(W_hx) \\\\\n",
        "o &= W_oh + x \\\\\n",
        "L(y, o) &= (y - o)^2 \\\\\n",
        "\\end{align}\n",
        "where $x$ is the input, $y \\in \\mathbb{R}$ is the scalar target, $W_h$ and $W_o$ are learnable parameters, and $o \\in \\mathbb{R}$ is the scalar output of the model.\n",
        "\n",
        "1. Derive the expressions for the gradient of the loss with respect to every parameter in the model (that is, $\\frac{\\partial L}{\\partial W_o}$ and $\\frac{\\partial L}{\\partial W_h}$).\n",
        "1. Re-express the model as a computational graph. The graph can have the following operations: Dot product, addition (not subtraction), elementwise maximum (ReLU), elementwise multiplication, and elementwise power (e.g. squaring). Your graph node should have leaf nodes corresponding to parameters and inputs.\n",
        "1. Specify which nodes in your graph have an output that needs to be cached in order to use the backpropagaion algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I3N0rIfCN55"
      },
      "source": [
        "# Problem 3 (3 points)\n",
        "\n",
        "Consider a convolutional neural network with the following structure:\n",
        "- Input image of size 32x32x3 (32 width, 32 height, 3 channels)\n",
        "- Convolutional layer with a 5x5 filter, 32 channels, stride of 1x1, and padding of 2x2\n",
        "- ReLU nonlinearity\n",
        "- Batch normalization\n",
        "- Max pooling with a 2x2 window, stride of 2x2, and no padding\n",
        "- Convolutional layer with a 3x3 filter, 64 channels, stride of 1x1, and padding of 1x1\n",
        "- ReLU nonlinearity\n",
        "- Batch normalization\n",
        "- Max pooling with a 2x2 window, stride of 2x2, and no padding\n",
        "- Convolutional layer with a 1x1 filter, 100 channels, stride of 1x1, and no padding\n",
        "\n",
        "1. What is the output shape (dimensionality/size of the activations) at the output of these layers?\n",
        "1. What is the receptive field of the input for one of the activations at the output of the model? Recall that the receptive field means the region of the input that influences a given activation.\n",
        "1. Assume that we remove all padding from all convolutional layers in the model. What is the output shape of these layers now?\n",
        "1. Now, instead assume that we remove the batch normalization and change the nonlinearities to be sigmoid instead of ReLU. What is the output shape of these layers now?\n",
        "1. Say we want to use this model for 10-class classification, so we add a single classification layer (i.e. a softmax regression model) to the output. How many parameters will this classification layer have?\n",
        "1. Imagine instead that we add a global average pooliong layer on the output of the layer stack before the 10-way softmax classification layer. How many parameters will the classification layer have now?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srmde5SAW9Yc"
      },
      "source": [
        "# Problem 4 (2 points)\n",
        "\n",
        "1. In the olden days of neural networks, it was popular to use the following nonlinearity: $$\\phi(x) = \\begin{cases} 0, x < 0\\\\1, x \\ge 0\\end{cases}$$ Can you foresee any issue with using gradient descent to train a neural network that uses this nonlinearity?\n",
        "1. An alternative to the ReLU nonlinearity is the softplus: $$\\phi(x) = \\log(1 + \\exp(x))$$ What is the gradient of this nonlinearity with respect to x? Name one advantage and one disadvantage of this nonlinearity over ReLU."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5 (4 points)\n",
        "\n",
        "A less-popular alternative to batch and layer normalization is weight normalization, which replaces each weight vector $w$ in the model with $\\frac{\\gamma}{||v||}v$ where $\\gamma$ is a new scalar parameter and $v$ is a new vector of parameters with the same shape as $w$.\n",
        "\n",
        "1. Derive expressions for $\\nabla_\\gamma L$ and $\\nabla_v L$ in terms of $\\gamma$, $v$, and $\\nabla_w L$.\n",
        "\n",
        "2. Show that applying batch normalization *without the shift parameter $\\beta$* to the preactivation $vx$ is equivalent to applying weight normalization if the entries of $x$ are independently distributed with zero mean and unit variance.\n"
      ],
      "metadata": {
        "id": "tTHhP6gyUYgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 6 (3 points)\n",
        "\n",
        "Consider the Adam optimizer and assume that $g_t = g \\ne 0$, i.e. the gradient is the same value $g \\ne 0$ at all iterations of training.\n",
        "\n",
        "1. Assume that $\\epsilon = 0$, $0 < \\beta < 1$, and $0 < \\beta_2 < 1$ and show that $g^\\prime_t = \\eta\\, \\text{sign}(g)$.\n",
        "2. Now, consider the case when $\\epsilon$ is large, e.g. $\\epsilon = 1$. How does the function for $g^\\prime_t$ change?"
      ],
      "metadata": {
        "id": "8sWGsPpsUaEY"
      }
    }
  ]
}